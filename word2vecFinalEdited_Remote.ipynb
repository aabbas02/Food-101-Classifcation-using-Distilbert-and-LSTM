{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtual Environments\\food101\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.18.0\n",
      "WARNING:tensorflow:From c:\\virtual Environments\\food101\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\virtual Environments\\food101\\Lib\\site-packages\\torchtext\\__init__.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\\n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \\n\"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#from gensim.models import word2vec\u001b[39;00m\n",
      "File \u001b[1;32mc:\\virtual Environments\\food101\\Lib\\site-packages\\torchtext\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\virtual Environments\\food101\\Lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\virtual Environments\\food101\\Lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32mc:\\virtual Environments\\food101\\Lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\virtual Environments\\food101\\Lib\\site-packages\\torch\\_ops.py:1350\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m   1345\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m   1347\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m-> 1350\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\ctypes\\__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%pip list\n",
    "\n",
    "import sys\n",
    "#from gensim.utils import tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import transformers\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import re\n",
    "#try:\n",
    "#    %tensorflow_version 2.x\n",
    "#except Exception:\n",
    "#    pass\n",
    "#import tensorflow as tf\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "import tensorflow_hub as hub\n",
    "from utils import *\n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras import optimizers\n",
    "#from tensorflow.keras import callbacks\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#import bert\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import gensim\n",
    "import sys\n",
    "import string\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torchtext\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall torchtext\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['image_path', 'text', 'food']\n",
    "train = pd.read_csv('train_titles.csv', names=colnames, header=None, sep = ',', index_col=['image_path'])\n",
    "test = pd.read_csv('test_titles.csv', names=colnames, header=None, sep = ',', index_col=['image_path'])\n",
    "# Sort values by 'image_path'\n",
    "test = test.sort_values('image_path')\n",
    "train = train.sort_values('image_path')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes\n",
    "print(\"train samples:\",train.shape[0])\n",
    "print(\"test samples:\",test.shape[0])\n",
    "vec_preprocess_text = np.vectorize(preprocess_text)\n",
    "processed_train = vec_preprocess_text(train.text.values.tolist() + test.text.values.tolist())\n",
    "print(train.text.values.tolist()[0])\n",
    "print(processed_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchtext -U\n",
    "%pip list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Length Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listData = [list(tokenize(processed_train[i])) for i in range(processed_train.shape[0])]\n",
    "#lengths = [len(listData[i]) for i in range(len(listData))]\n",
    "#fig,ax = plt.subplots(figsize = (7,5))\n",
    "#ax.hist(lengths,bins = range(0,30,1))\n",
    "#ax.set_ylabel('Counts', fontsize = 11)\n",
    "#ax.set_xlabel('Number of words', fontsize =11)\n",
    "#ax.set_title('Sentence lengths in input data', fontsize = 11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Word2vec on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dim = 100                 \n",
    "w2vModel = word2vec.Word2Vec(listData, vector_size=dim,min_count=1)\n",
    "# add default key, that is, value for unknown key, may happen during testing\n",
    "w2vModel.wv.add_vector(0,np.zeros((dim)))\n",
    "w2v_weights = w2vModel.wv.vectors\n",
    "\n",
    "print(w2vModel.wv.most_similar('barbecue',topn=3))\n",
    "print(w2vModel.wv.most_similar('apple',topn=3))\n",
    "print(w2vModel.wv.most_similar('falafel',topn=3))\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "try:\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "except IOError:\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def build_vocab(data, tokenizer):\n",
    "    counter = Counter()\n",
    "    for string_ in data:\n",
    "        counter.update(tokenizer.tokenizer(str(string_)))\n",
    "    \n",
    "    return Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "vocab = build_vocab(processed_train, spacy_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vocab.__len__() )\n",
    "print(vocab.itos[5])\n",
    "dict = vocab.stoi\n",
    "dict[vocab.itos[5]]\n",
    "#print(vocab.itos[5].type)\n",
    "vocab.set_default_index(vocab['<unk>'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Captions to Tokens to IDs and Assign Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2idNew(listData, maxLen, vocab):\n",
    "    ids = torch.zeros( (len(listData),maxLen) )\n",
    "    # set equal to id of 0/pad vector\n",
    "    ids[:,:] = torch.tensor(w2vModel.wv.key_to_index[0])\n",
    "    for i in range(len(listData)):\n",
    "        for j in range(min([maxLen,len(listData[i])])):\n",
    "            ids[i,j] = torch.tensor(w2vModel.wv.key_to_index[listData[i][j]])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClasses = 101\n",
    "# optionally downsample by reducing the number of classes \n",
    "#train,test = downSampleData(numClasses, train, test)\n",
    "countsTrn,countsTst  = classCounts(numClasses, train, test)\n",
    "processed_train = vec_preprocess_text(train.text.values)\n",
    "processed_test = vec_preprocess_text(test.text.values)\n",
    "#print(processed_train[0])\n",
    "#print(processed_test[0])\n",
    "# get train and test labels\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels_train = encoder.fit_transform(train.food.values)\n",
    "encoded_labels_test = encoder.fit_transform(test.food.values)\n",
    "# map train and test tokens to ids\n",
    "maxLen = 32\n",
    "#processed_train = [list(tokenize(processed_train[i])) for i in range(processed_train.shape[0])]\n",
    "#idsTrn = token2id(processed_train, maxLen, w2vModel)\n",
    "idsTrn = token2idNew(processed_train,maxLen, vocab)\n",
    "#processed_test = [list(tokenize(processed_test[i])) for i in range(processed_test.shape[0])]\n",
    "#idsTst = token2id(processed_test, maxLen, w2vModel)\n",
    "idsTst = token2idNew(processed_test,maxLen, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2nModel(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_size, nClasses):\n",
    "        super(w2nModel, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size,batch_first=True)\n",
    "        #self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.drop1 = nn.Dropout(p = 0.5)\n",
    "        self.FC1 = nn.Linear(hidden_size,256)\n",
    "        self.drop2 = nn.Dropout(p = 0.5)\n",
    "        self.FC2 = nn.Linear(256,nClasses)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x) # output dimensions is batch size = N x sequence length x feature size\n",
    "        (x,_) = self.LSTM(x)        \n",
    "        x = x[:, -1, :] # gives two dimensional output, not three dimensional output\n",
    "        #x = self.bn(x)\n",
    "        x = self.drop1(x)               \n",
    "        x = self.Relu(self.FC1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.FC2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "torch.device('cuda:0')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#device = 'gpu'\n",
    "hidden_size = 64\n",
    "model = w2nModel(vocab_size = w2v_weights.shape[0], \n",
    "                 embedding_dim = w2v_weights.shape[1],\n",
    "                 hidden_size=hidden_size, nClasses = numClasses\n",
    "                )\n",
    "#model.word_embeddings.weight.data.copy_(torch.from_numpy(w2v_weights))\n",
    "#model.word_embeddings.weight.requires_grad=False\n",
    "\n",
    "labels_train = torch.tensor(encoded_labels_train).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTrn.int(), labels_train)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "lossVals = []\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs,labels = data\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        lossVals.append(loss.detach().cpu().numpy())\n",
    "        optimizer.step()\n",
    "    if epoch%5 == 0:\n",
    "        print(loss)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = torch.tensor(encoded_labels_test).long()\n",
    "testLoader = torch.utils.data.DataLoader(list(zip(idsTst.int(), labels_test)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "# again no gradients needed\n",
    "model.eval()\n",
    "correct_pred = 0\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred += 1\n",
    "\n",
    "accuracy = 100 * float(correct_pred)/ idsTst.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols=1)\n",
    "ax.plot(range(len(lossVals)),lossVals)\n",
    "ax.set_xlabel('Iterations',fontsize = 15)\n",
    "ax.set_ylabel('Cross Entropy Loss', fontsize = 15)\n",
    "ax.set_title('Classification Accuracy = {:.2f}%'.format(accuracy),fontsize = 15)\n",
    "path = 'dim_{}_accry_{:.2f}len_{}_hidden_{}'.format(dim,accuracy, maxLen,hidden_size)\n",
    "plt.savefig(path+'.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 5 Most Error-Full Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "checkpoint_path = path+'\\_accrcy_{:.2f}_dim_{}.pt'.format(accuracy,dim)\n",
    "print(checkpoint_path)\n",
    "torch.save(model, checkpoint_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2nModelSVM(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_size, nClasses):\n",
    "        super(w2nModelSVM, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size,batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)     # output dimensions is batch size = N x sequence length x feature size\n",
    "        (x,_) = self.LSTM(x)        \n",
    "        x = x[:, -1, :]                 # gives two dimensional output, not three dimensional output\n",
    "        return x\n",
    "\n",
    "torch.device('cuda:0')\n",
    "modelSVM = w2nModelSVM(vocab_size = w2v_weights.shape[0], \n",
    "                 embedding_dim = w2v_weights.shape[1],\n",
    "                 hidden_size=hidden_size, nClasses = numClasses\n",
    "                )\n",
    "modelSVM.word_embeddings.weight.data.copy_(torch.from_numpy(w2v_weights))\n",
    "modelSVM.word_embeddings.weight.requires_grad=False\n",
    "modelSVM.LSTM.load_state_dict(model.LSTM.state_dict())\n",
    "modelSVM.LSTM.requires_grad_=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get LSTM Embeddings for Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = torch.tensor(encoded_labels_train).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTrn.int(), labels_train)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "trnEmbdngs = np.zeros((idsTrn.shape[0],hidden_size))\n",
    "trnLbls  = np.zeros((idsTrn.shape[0]))\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs,labels = data\n",
    "    outputs = modelSVM(inputs)\n",
    "    trnEmbdngs[i*batchSize: (i+1)*batchSize,:] = outputs.detach().clone().numpy()\n",
    "    trnLbls[i*batchSize: (i+1)*batchSize] = labels\n",
    "\n",
    "labels_test = torch.tensor(encoded_labels_test).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTst.int(), labels_test)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "tstEmbdngs = np.zeros((idsTst.shape[0],hidden_size))\n",
    "tstLbls  = np.zeros((idsTst.shape[0]))\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs,labels = data\n",
    "    outputs = modelSVM(inputs)\n",
    "    tstEmbdngs[i*batchSize: (i+1)*batchSize,:] = outputs.detach().clone().numpy()\n",
    "    tstLbls[i*batchSize: (i+1)*batchSize] = labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(decision_function_shape='ovo', kernel = 'linear')\n",
    "clf.fit(trnEmbdngs, trnLbls)\n",
    "TrnAccrcyLnr = clf.score(trnEmbdngs, trnLbls)\n",
    "TstAccrcyLnr = clf.score(tstEmbdngs,tstLbls)\n",
    "print(r'Train Accuracy of Linear SVM =', TrnAccrcyLnr)\n",
    "print(TstAccrcyLnr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo', kernel='rbf')\n",
    "clf.fit(trnEmbdngs, trnLbls)\n",
    "TrnAccrcyKernel = clf.score(trnEmbdngs, trnLbls)\n",
    "TstAccrcyKernel = clf.score(tstEmbdngs,tstLbls)\n",
    "print(TrnAccrcyKernel)\n",
    "print(TstAccrcyKernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM without LSTM - Averaging Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2nModelSVM(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_size, nClasses):\n",
    "        super(w2nModelSVM, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)                # output dimensions is batch size = N x sequence length x feature size\n",
    "        x = torch.sum(x,dim=1)/x.shape[1]          # batch size x feature size\n",
    "        return x\n",
    "\n",
    "torch.device('cuda:0')\n",
    "modelw2vAvg = w2nModelSVM(vocab_size = w2v_weights.shape[0], \n",
    "                 embedding_dim = w2v_weights.shape[1],\n",
    "                 hidden_size=hidden_size, nClasses = numClasses\n",
    "                )\n",
    "modelw2vAvg.word_embeddings.weight.data.copy_(torch.from_numpy(w2v_weights))\n",
    "modelw2vAvg.word_embeddings.weight.requires_grad=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = torch.tensor(encoded_labels_train).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTrn.int(), labels_train)),\n",
    "                                         batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "trnEmbdngs = np.zeros((idsTrn.shape[0],dim))\n",
    "trnLbls  = np.zeros((idsTrn.shape[0]))\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs,labels = data\n",
    "    outputs = modelw2vAvg(inputs)\n",
    "    trnEmbdngs[i*batchSize: (i+1)*batchSize,:] = outputs.detach().clone().numpy()\n",
    "    trnLbls[i*batchSize: (i+1)*batchSize] = labels\n",
    "\n",
    "labels_test = torch.tensor(encoded_labels_test).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTst.int(), labels_test)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "tstEmbdngs = np.zeros((idsTst.shape[0],dim))\n",
    "tstLbls  = np.zeros((idsTst.shape[0]))\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs,labels = data\n",
    "    outputs = modelw2vAvg(inputs)\n",
    "    tstEmbdngs[i*batchSize: (i+1)*batchSize,:] = outputs.detach().clone().numpy()\n",
    "    tstLbls[i*batchSize: (i+1)*batchSize] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo', kernel = 'linear')\n",
    "clf.fit(trnEmbdngs, trnLbls)\n",
    "TrnAccrcyLnr = clf.score(trnEmbdngs, trnLbls)\n",
    "TstAccrcyLnr = clf.score(tstEmbdngs,tstLbls)\n",
    "print(TrnAccrcyLnr)\n",
    "print(TstAccrcyLnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo', kernel='rbf')\n",
    "clf.fit(trnEmbdngs, trnLbls)\n",
    "TrnAccrcyKernel = clf.score(trnEmbdngs, trnLbls)\n",
    "TstAccrcyKernel = clf.score(tstEmbdngs,tstLbls)\n",
    "print(TrnAccrcyKernel)\n",
    "print(TstAccrcyKernel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
