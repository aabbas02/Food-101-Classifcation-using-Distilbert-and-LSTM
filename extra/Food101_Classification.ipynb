{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from gensim.utils import tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import transformers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import re\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "import tensorflow_hub as hub\n",
    "from utils import *\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import callbacks\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#import bert\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gensim\n",
    "import sys\n",
    "import string\n",
    "import logging\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (1.26.4)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: plotly in c:\\python312\\lib\\site-packages (5.24.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python312\\lib\\site-packages (from gensim) (7.0.5)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (70.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached grpcio-1.68.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached h5py-3.12.1-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\python312\\lib\\site-packages (from plotly) (9.0.0)\n",
      "Requirement already satisfied: filelock in c:\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.6.2)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aabbasi1\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Using cached tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "Using cached huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.68.0-cp312-cp312-win_amd64.whl (4.4 MB)\n",
      "Using cached h5py-3.12.1-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "Using cached keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Installing collected packages: joblib, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, scikit-learn, huggingface-hub, tokenizers, keras, transformers, tensorflow-intel, tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python312\\\\Scripts\\\\tensorboard.exe' -> 'c:\\\\Python312\\\\Scripts\\\\tensorboard.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim numpy tensorflow plotly transformers scikit-learn tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['image_path', 'text', 'food']\n",
    "train = pd.read_csv('train_titles.csv', names=colnames, header=None, sep = ',', index_col=['image_path'])\n",
    "test = pd.read_csv('test_titles.csv', names=colnames, header=None, sep = ',', index_col=['image_path'])\n",
    "# Sort values by 'image_path'\n",
    "test = test.sort_values('image_path')\n",
    "train = train.sort_values('image_path')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes\n",
    "print(\"train samples:\",train.shape[0])\n",
    "print(\"test samples:\",test.shape[0])\n",
    "vec_preprocess_text = np.vectorize(preprocess_text)\n",
    "processed_train = vec_preprocess_text(train.text.values.tolist() + test.text.values.tolist())\n",
    "print(train.text.values.tolist()[0])\n",
    "print(processed_train[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Length Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listData = [list(tokenize(processed_train[i])) for i in range(processed_train.shape[0])]\n",
    "lengths = [len(listData[i]) for i in range(len(listData))]\n",
    "fig,ax = plt.subplots(figsize = (7,5))\n",
    "ax.hist(lengths,bins = range(0,30,1))\n",
    "ax.set_ylabel('Counts', fontsize = 11)\n",
    "ax.set_xlabel('Number of words', fontsize =11)\n",
    "ax.set_title('Sentence lengths in input data', fontsize = 11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Word2vec on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100                 \n",
    "w2vModel = word2vec.Word2Vec(listData, vector_size=dim,min_count=1)\n",
    "# add default key, that is, value for unknown key, may happen during testing\n",
    "w2vModel.wv.add_vector(0,np.zeros((dim)))\n",
    "w2v_weights = w2vModel.wv.vectors\n",
    "\n",
    "print(w2vModel.wv.most_similar('barbecue',topn=3))\n",
    "print(w2vModel.wv.most_similar('apple',topn=3))\n",
    "print(w2vModel.wv.most_similar('falafel',topn=3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Captions to Tokens to IDs and Assign Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numClasses = 101\n",
    "# optionally downsample by reducing the number of classes \n",
    "#train,test = downSampleData(numClasses, train, test)\n",
    "countsTrn,countsTst  = classCounts(numClasses, train, test)\n",
    "processed_train = vec_preprocess_text(train.text.values)\n",
    "processed_test = vec_preprocess_text(test.text.values)\n",
    "print(processed_train[0])\n",
    "print(processed_test[0])\n",
    "# get train and test labels\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels_train = encoder.fit_transform(train.food.values)\n",
    "encoded_labels_test = encoder.fit_transform(test.food.values)\n",
    "# map train and test tokens to ids\n",
    "maxLen = 32\n",
    "processed_train = [list(tokenize(processed_train[i])) for i in range(processed_train.shape[0])]\n",
    "idsTrn = token2id(processed_train, maxLen, w2vModel)\n",
    "processed_test = [list(tokenize(processed_test[i])) for i in range(processed_test.shape[0])]\n",
    "idsTst = token2id(processed_test, maxLen, w2vModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2nModel(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_size, nClasses):\n",
    "        super(w2nModel, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size,batch_first=True)\n",
    "        #self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        self.drop1 = nn.Dropout(p = 0.5)\n",
    "        self.FC1 = nn.Linear(hidden_size,256)\n",
    "        self.drop2 = nn.Dropout(p = 0.5)\n",
    "        self.FC2 = nn.Linear(256,nClasses)\n",
    "        self.Relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x) # output dimensions is batch size = N x sequence length x feature size\n",
    "        (x,_) = self.LSTM(x)        \n",
    "        x = x[:, -1, :] # gives two dimensional output, not three dimensional output\n",
    "        #x = self.bn(x)\n",
    "        x = self.drop1(x)               \n",
    "        x = self.Relu(self.FC1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.FC2(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "torch.device('cuda:0')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#device = 'gpu'\n",
    "batchSize = 512\n",
    "hidden_size = 64\n",
    "model = w2nModel(vocab_size = w2v_weights.shape[0], \n",
    "                 embedding_dim = w2v_weights.shape[1],\n",
    "                 hidden_size=hidden_size, nClasses = numClasses\n",
    "                )\n",
    "#model.word_embeddings.weight.data.copy_(torch.from_numpy(w2v_weights))\n",
    "#model.word_embeddings.weight.requires_grad=False\n",
    "\n",
    "labels_train = torch.tensor(encoded_labels_train).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTrn.int(), labels_train)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "lossVals = []\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs,labels = data\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        lossVals.append(loss.detach().cpu().numpy())\n",
    "        optimizer.step()\n",
    "    if epoch%5 == 0:\n",
    "        print(loss)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = torch.tensor(encoded_labels_test).long()\n",
    "testLoader = torch.utils.data.DataLoader(list(zip(idsTst.int(), labels_test)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "# again no gradients needed\n",
    "model.eval()\n",
    "correct_pred = 0\n",
    "with torch.no_grad():\n",
    "    for data in testLoader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device) \n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred += 1\n",
    "\n",
    "accuracy = 100 * float(correct_pred)/ idsTst.shape[0]\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols=1)\n",
    "ax.plot(range(len(lossVals)),lossVals)\n",
    "ax.set_xlabel('Iterations',fontsize = 15)\n",
    "ax.set_ylabel('Cross Entropy Loss', fontsize = 15)\n",
    "ax.set_title('Classification Accuracy = {:.2f}%'.format(accuracy),fontsize = 15)\n",
    "path = 'dim_{}_accry_{:.2f}len_{}_hidden_{}'.format(dim,accuracy, maxLen,hidden_size)\n",
    "plt.savefig(path+'.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 5 Most Error-Full Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "checkpoint_path = path+'\\_accrcy_{:.2f}_dim_{}.pt'.format(accuracy,dim)\n",
    "print(checkpoint_path)\n",
    "torch.save(model, checkpoint_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2nModelSVM(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_size, nClasses):\n",
    "        super(w2nModelSVM, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size,batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)     # output dimensions is batch size = N x sequence length x feature size\n",
    "        (x,_) = self.LSTM(x)        \n",
    "        x = x[:, -1, :]                 # gives two dimensional output, not three dimensional output\n",
    "        return x\n",
    "\n",
    "torch.device('cuda:0')\n",
    "modelSVM = w2nModelSVM(vocab_size = w2v_weights.shape[0], \n",
    "                 embedding_dim = w2v_weights.shape[1],\n",
    "                 hidden_size=hidden_size, nClasses = numClasses\n",
    "                )\n",
    "modelSVM.word_embeddings.weight.data.copy_(torch.from_numpy(w2v_weights))\n",
    "modelSVM.word_embeddings.weight.requires_grad=False\n",
    "modelSVM.LSTM.load_state_dict(model.LSTM.state_dict())\n",
    "modelSVM.LSTM.requires_grad_=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get LSTM Embeddings for Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = torch.tensor(encoded_labels_train).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTrn.int(), labels_train)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "trnEmbdngs = np.zeros((idsTrn.shape[0],hidden_size))\n",
    "trnLbls  = np.zeros((idsTrn.shape[0]))\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs,labels = data\n",
    "    outputs = modelSVM(inputs)\n",
    "    trnEmbdngs[i*batchSize: (i+1)*batchSize,:] = outputs.detach().clone().numpy()\n",
    "    trnLbls[i*batchSize: (i+1)*batchSize] = labels\n",
    "\n",
    "labels_test = torch.tensor(encoded_labels_test).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTst.int(), labels_test)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "tstEmbdngs = np.zeros((idsTst.shape[0],hidden_size))\n",
    "tstLbls  = np.zeros((idsTst.shape[0]))\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs,labels = data\n",
    "    outputs = modelSVM(inputs)\n",
    "    tstEmbdngs[i*batchSize: (i+1)*batchSize,:] = outputs.detach().clone().numpy()\n",
    "    tstLbls[i*batchSize: (i+1)*batchSize] = labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(decision_function_shape='ovo', kernel = 'linear')\n",
    "clf.fit(trnEmbdngs, trnLbls)\n",
    "TrnAccrcyLnr = clf.score(trnEmbdngs, trnLbls)\n",
    "TstAccrcyLnr = clf.score(tstEmbdngs,tstLbls)\n",
    "print(r'Train Accuracy of Linear SVM =', TrnAccrcyLnr)\n",
    "print(TstAccrcyLnr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo', kernel='rbf')\n",
    "clf.fit(trnEmbdngs, trnLbls)\n",
    "TrnAccrcyKernel = clf.score(trnEmbdngs, trnLbls)\n",
    "TstAccrcyKernel = clf.score(tstEmbdngs,tstLbls)\n",
    "print(TrnAccrcyKernel)\n",
    "print(TstAccrcyKernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM without LSTM - Averaging Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2nModelSVM(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, hidden_size, nClasses):\n",
    "        super(w2nModelSVM, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embeddings(x)                # output dimensions is batch size = N x sequence length x feature size\n",
    "        x = torch.sum(x,dim=1)/x.shape[1]          # batch size x feature size\n",
    "        return x\n",
    "\n",
    "torch.device('cuda:0')\n",
    "modelw2vAvg = w2nModelSVM(vocab_size = w2v_weights.shape[0], \n",
    "                 embedding_dim = w2v_weights.shape[1],\n",
    "                 hidden_size=hidden_size, nClasses = numClasses\n",
    "                )\n",
    "modelw2vAvg.word_embeddings.weight.data.copy_(torch.from_numpy(w2v_weights))\n",
    "modelw2vAvg.word_embeddings.weight.requires_grad=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = torch.tensor(encoded_labels_train).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTrn.int(), labels_train)),\n",
    "                                         batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "trnEmbdngs = np.zeros((idsTrn.shape[0],dim))\n",
    "trnLbls  = np.zeros((idsTrn.shape[0]))\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs,labels = data\n",
    "    outputs = modelw2vAvg(inputs)\n",
    "    trnEmbdngs[i*batchSize: (i+1)*batchSize,:] = outputs.detach().clone().numpy()\n",
    "    trnLbls[i*batchSize: (i+1)*batchSize] = labels\n",
    "\n",
    "labels_test = torch.tensor(encoded_labels_test).long()\n",
    "trainloader = torch.utils.data.DataLoader(list(zip(idsTst.int(), labels_test)), batch_size=batchSize,\n",
    "                                         shuffle=True)\n",
    "tstEmbdngs = np.zeros((idsTst.shape[0],dim))\n",
    "tstLbls  = np.zeros((idsTst.shape[0]))\n",
    "for i, data in enumerate(trainloader):\n",
    "    inputs,labels = data\n",
    "    outputs = modelw2vAvg(inputs)\n",
    "    tstEmbdngs[i*batchSize: (i+1)*batchSize,:] = outputs.detach().clone().numpy()\n",
    "    tstLbls[i*batchSize: (i+1)*batchSize] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo', kernel = 'linear')\n",
    "clf.fit(trnEmbdngs, trnLbls)\n",
    "TrnAccrcyLnr = clf.score(trnEmbdngs, trnLbls)\n",
    "TstAccrcyLnr = clf.score(tstEmbdngs,tstLbls)\n",
    "print(TrnAccrcyLnr)\n",
    "print(TstAccrcyLnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(decision_function_shape='ovo', kernel='rbf')\n",
    "clf.fit(trnEmbdngs, trnLbls)\n",
    "TrnAccrcyKernel = clf.score(trnEmbdngs, trnLbls)\n",
    "TstAccrcyKernel = clf.score(tstEmbdngs,tstLbls)\n",
    "print(TrnAccrcyKernel)\n",
    "print(TstAccrcyKernel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
